{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3J3UMwkFk4Y"
   },
   "source": [
    "# Multiple Linear Regression (Part I): Predicting Insurance Premiums\n",
    "\n",
    "This jupyter notebook discusses the creation and interpretation of regression models in both the descriptive and predictive settings. This notebook and the corresponding assignments (see BrightSpace) should take you about a week to work through -- please contact me with questions you have. The notebook comes in two parts, with a third part coming next week.\n",
    "1. **Exploratory data analysis:** This should be familiar to you. We explore the potential for relationships between variables by making heavy use of data visualization. This is a first step in any model-building effort.\n",
    "2. **Regression:** About halfway through this notebook you'll find the heading \"Our First Regression Model\". Here you'll find a discussion and walkthrough for constructing regression models for descriptive purposes as well as predictive purposes. \n",
    "  - You'll want to adapt the code I am providing here to some datasets of your choosing to ensure that you are able to construct your own regression models. You can find your own datasets on Kaggle and I'll also post some pre-approved datasets for those of you who would prefer some suggested data to work with.\n",
    "\n",
    "## About Our Data\n",
    "\n",
    "- Our simple dataset contains a few attributes for each person such as \n",
    "- Age, Sex, BMI, Children, Smoker, Region and their charges\n",
    "\n",
    "## Goal\n",
    "- To use this info to predict charges for new customers\n",
    "  - An insurance company can use this information to inform *rate-setting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3xySfs8Fjj7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "file_name = \"https://raw.githubusercontent.com/rajeevratan84/datascienceforbusiness/master/insurance.csv\"\n",
    "insurance = pd.read_csv(file_name)\n",
    "\n",
    "# Preview our data\n",
    "insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lhFaFp3a7WR"
   },
   "source": [
    "We'll use the `.info()` method to see how python is thinking about the features of our dataset (numeric versus categorical). We can also use the `.isna().sum()` chain to determine if and where missing values exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DilydrqOdGe1"
   },
   "outputs": [],
   "source": [
    "insurance.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYfyzYjmAcVr"
   },
   "source": [
    "Notice that `sex`, `smoker`, and `region` are all categorical (signified by the *object* tag), while the other variables are numerical. We don't need to pay attention to the difference between *int* and *float* here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0krXm5HhdGe4"
   },
   "outputs": [],
   "source": [
    "insurance.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7IXT2A0bn1M"
   },
   "source": [
    "We've got a dataset wit no missing values again -- how lucky! You should note, however, that datasets as nice as the ones we've been working with are rare. They are chosen for class discussions because they allow us to focus on \"the task at hand\" -- here we simply focus on a regression application, bypassing any cleaning, tidying, or imputing. You should beware that the tidying process can take upwards of 80% of the time on any analytics project -- we are significantly downplaying that aspect of analytics projects throughout our discussions.\n",
    "\n",
    "In addition to the simple exploration we've done so far, we can use the `print()` function to organize some other meaningful characteristics of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faXdZrMDdGe8"
   },
   "outputs": [],
   "source": [
    "print (\"Rows     : \" , insurance.shape[0])\n",
    "print (\"Columns  : \" , insurance.shape[1])\n",
    "print (\"\\nFeatures : \\n\" , insurance.columns.tolist())\n",
    "print (\"\\nMissing values :  \", insurance.isnull().sum().values.sum())\n",
    "print (\"\\nUnique values :  \\n\",insurance.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EFZgQ8AcDvx"
   },
   "source": [
    "## Train/Test/Safe\n",
    "\n",
    "Before moving forward and actually looking at the data, we should break our dataset into the *train*, *test*, and *safe* sets so that we are able to validate our findings and compare our models. We'll do this with `train_test_split`, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgKTuIGicYZ4"
   },
   "outputs": [],
   "source": [
    "X = insurance.drop([\"charges\"], axis = 1)\n",
    "y = insurance[\"charges\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.25, random_state = 370)\n",
    "X_safe, X_test, y_safe, y_test = train_test_split(X_temp, y_temp, test_size = 0.6, random_state = 570)\n",
    "\n",
    "print(y_train.head())\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k09SHsnudQnQ"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Now that we have the data split, we'll proceed with the training data (`X_train` and `y_train`) and then perform validations and comparisons with the hidden data from `X_test` and `y_test`. A reasonable first step is to look for strong correlations between `charges` and the potential numerical predictors (`age`, `bmi`, and `children`). We can do this numerically or also visually. Since it will be useful for our exploratory analyses, we'll start by joining `X_train` and `y_train` together in a single data frame called `train`. Once we have the single data frame, we'll compute the correlations between pairs of numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjNAXsBwdGe_"
   },
   "outputs": [],
   "source": [
    "train = X_train.copy()\n",
    "train[\"charges\"] = y_train\n",
    "train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GUhDbFv30RI"
   },
   "source": [
    "A numerical summary is nice, but sometimes a picture can more easily convey information -- especially to people who might consider themselves as *non-quantitative* individuals. In the code block below we write and use a simple function to convert the correlation matrix above into a visual plot of colored tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_TWJFB4dGfD"
   },
   "outputs": [],
   "source": [
    "def plot_corr(df,size=10):\n",
    "    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n",
    "\n",
    "    Input:\n",
    "        df: pandas DataFrame\n",
    "        size: vertical and horizontal size of the plot'''\n",
    "\n",
    "    corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.legend()\n",
    "    cax = ax.matshow(corr)\n",
    "    fig.colorbar(cax)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical')\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    \n",
    "plot_corr(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWeHm2u8ewbE"
   },
   "source": [
    "It looks like `age` and `bmi` correlate most strongly with the `charges` variable. Let's look a bit more at each of the features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOthDwjWdGfG"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1) #add a plot to a 2x2 array of plots, in position 1 (top-left)\n",
    "plt.hist(x=train[\"age\"], bins=70, color=\"b\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(2, 2, 2) #add a plot to a 2x2 array of plots, in position 2 (top-right)\n",
    "plt.hist(x=train[\"bmi\"], bins=100, color=\"r\")\n",
    "plt.xlabel(\"Body Mass Index (BMI)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(2, 2, 3) #add a plot to a 2x2 array of plots, in position 3 (bottom-left)\n",
    "plt.hist(x=train[\"children\"], bins=6, color=\"g\")\n",
    "plt.xlabel(\"Number of Children\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(2, 2, 4) #add a plot to a 2x2 array of plots, in position 4 (bottom-right)\n",
    "plt.hist(x=train[\"charges\"], bins=100, color=\"orange\")\n",
    "plt.xlabel(\"Overall Charges\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1VWyI63faoK"
   },
   "source": [
    "From the plots we can see that the `age` is distributed relatively uniformly for people at least 20 years old -- there are a disproportionate number of records with young people (under 20). If we had access to someone from this firm, we would ask for clarification on why that is. The `bmi` variable seems approximaely normal with a possible slight right skew. The `children` and `charges` variables are both strongly right skewed. It may make sense to consider a categorical version of the `children` variable. In `charges` we almost seem to have three different groupings -- the majority which are below about \\\\$14,000, a second group which is between \\\\$15,000 and about \\\\$31,000, and then a third group which is very expensive -- it looks like typically between \\\\$32,000 and \\\\$50,000, with a few outliers at the top. Again, with access to a company representative, we would ask more about this.\n",
    "\n",
    "Now let's take a look at the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKxcSJ3BdGfI"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1) #add a plot to a 1x3 array of plots, in position 2 (left)\n",
    "sns.countplot(x = \"sex\", data = train)\n",
    "plt.subplot(1, 3, 2) #add a plot to a 1x3 array of plots, in position 2 (middle)\n",
    "sns.countplot(x = \"smoker\", data = train)\n",
    "plt.subplot(1, 3, 3) #add a plot to a 1x3 array of plots, in position 3 (right)\n",
    "sns.countplot(x = \"region\", data = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goLWVdWnhvjU"
   },
   "source": [
    "It looks like we have an approximately 50-50 gender split and the majority of our insureds are non-smokers. The distribution of `region` in which the insureds live is relatively uniform with a slightly larger proportion from the southeast. We saw earlier that the `children` variable was skewed right (expectedly) -- what if we engineered a new variable `hasChildren` with levels `none`, `oneToThree`, `moreThanThree`? Remember, **since we only want to do this once, we will use the original `insurance` dataset and then re-split**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCfaAX5vip04"
   },
   "outputs": [],
   "source": [
    "#Initialize the new column\n",
    "insurance[\"hasChildren\"] = 0*insurance.size\n",
    "\n",
    "#Populate the new column with the appropriate values\n",
    "insurance[\"hasChildren\"].loc[insurance[\"children\"] == 0] = \"None\"\n",
    "insurance[\"hasChildren\"].loc[(insurance[\"children\"] > 0) & (insurance[\"children\"] < 4)] = \"oneToThree\"\n",
    "insurance[\"hasChildren\"].loc[insurance[\"children\"] >= 4] = \"moreThanThree\"\n",
    "\n",
    "#Now we re-split\n",
    "X = insurance.drop([\"charges\"], axis = 1)\n",
    "y = insurance[\"charges\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.25, random_state = 370)\n",
    "X_safe, X_test, y_safe, y_test = train_test_split(X_temp, y_temp, test_size = 0.6, random_state = 570)\n",
    "\n",
    "#And we'll re-create the train data frame which includes the charges column.\n",
    "train = X_train.copy()\n",
    "train[\"charges\"] = y_train\n",
    "\n",
    "#print the head() of the training data to check that we have what we intended.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIm4UNWadGfM"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 8))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "sns.scatterplot(x = \"age\", y = \"charges\", data = train, alpha = 0.4)\n",
    "ax1 = fig.add_subplot(2, 3, 2)\n",
    "sns.scatterplot(x = \"bmi\", y = \"charges\", data = train, alpha = 0.3)\n",
    "ax1 = fig.add_subplot(2, 3, 3)\n",
    "sns.boxplot(x = \"sex\", y = \"charges\", data = train)\n",
    "ax1 = fig.add_subplot(2, 3, 4)\n",
    "sns.boxplot(x = \"hasChildren\", y = \"charges\", data = train)\n",
    "ax1 = fig.add_subplot(2, 3, 5)\n",
    "sns.boxplot(x = \"smoker\", y = \"charges\", data = train)\n",
    "ax1 = fig.add_subplot(2, 3, 6)\n",
    "sns.boxplot(x = \"region\", y = \"charges\", data = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm3ypaFHnJB-"
   },
   "source": [
    "While the plot on the lower middle is pretty telling, what could we see if we overlay the `smoker` variable with the scatterplots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_7DzDpEdGfP"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 8))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "sns.scatterplot(x=\"age\", y=\"charges\", data=insurance, hue='smoker', alpha = 0.4)\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "sns.scatterplot(x=\"bmi\", y=\"charges\", data=insurance, hue='smoker', alpha = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf4Se7rznzvX"
   },
   "source": [
    "It seems that an older, overweight, smoker is very expensive to insure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQMLESYrdGfR"
   },
   "outputs": [],
   "source": [
    "#sns.catplot(x=\"sex\", y=\"charges\", hue=\"smoker\", kind=\"violin\", data=train)\n",
    "sns.catplot(x=\"sex\", y=\"charges\", hue=\"smoker\", kind = \"violin\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUSM2OZVwKsU"
   },
   "source": [
    "We see that the genders are fairly comparable in terms of `charges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7waaY7DwdGfT"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(insurance, hue=\"smoker\")\n",
    "plt.title(\"Smokers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMdWcdOUweQV"
   },
   "source": [
    "The EDA confirms that smoking is a major predictor of high cost to insure. We even see that those two bumps in the distribution of `charges` correspond to smokers. Let's move forward and build a predictive model for insurance charges. This model will help in rate setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tksBjgTaJPWu"
   },
   "source": [
    "# Our First Regression Model\n",
    "\n",
    "What you've gone through so far is an initial exploratory analysis designed to give you insights into the possible relationships between insurance `charges` and characteristics of each corresponding policy holder. We haven't done any true statistical modeling -- that starts now!\n",
    "\n",
    "## Preparing Data for Machine Learning Algorithms\n",
    "\n",
    "In order to prepare our data for machine learning algorithms we'll need to re-code our categorical variables. The easiest way to do this is called *one hot* encoding. For a given categorical variable, we create a new *dummy variable* (think of this as a light switch) which can be turned on (1) or off (0) -- these dummy variabes can't take on values other than 0 or 1. We can always get away with one less dummy variable than there are levels to a categorical variable by defining a *base level* -- this is the default level in the case that all of the dummies are turned off. There are some drawbacks to this approach but doing anything more requires lots of domain expertise, assumption, and justification -- not to mention if done inappropriately can lead to catastrophic results. We'll stick to *one hot* encodings, but I can show you an example where we can improve on it during our discussions.\n",
    "\n",
    "Remember that, in order to do the one-hot encodings only once, we will do the encoding on the original `insurance` dataset and then re-split the data. Also, since we are using the `hasChildren` feature we engineered, we will drop the `children` column from the dataset since this is now duplicated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWY-Jaw-dGfZ"
   },
   "outputs": [],
   "source": [
    "##### WARNING: Running this block a second time will throw a warning because your \n",
    "##### insurance dataset will no longer have the hasChildren, sex, smoke, or \n",
    "##### region columns.\n",
    " \n",
    "# Obtain one-hot encodings for categorical variables.\n",
    "insurance = pd.get_dummies(insurance, columns = [\"hasChildren\", \"sex\", \"smoker\", \"region\"], drop_first= True)\n",
    "insurance.drop([\"children\"], axis = 1, inplace = True)\n",
    "#Re-split to training, test, and safe data\n",
    "X = insurance.drop([\"charges\"], axis = 1)\n",
    "y = insurance[\"charges\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.25, random_state = 370)\n",
    "X_safe, X_test, y_safe, y_test = train_test_split(X_temp, y_temp, test_size = 0.6, random_state = 570)\n",
    "\n",
    "#And we'll re-create the train data frame which includes the charges column.\n",
    "train = X_train.copy()\n",
    "train[\"charges\"] = y_train\n",
    "\n",
    "#print the head() of the training data to check that we have what we intended.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR0_gwp5C39e"
   },
   "source": [
    "As you can see, the `hasChildren`, `sex`, `smoker`, and `region` variables have been replaced by dummy variables indicating their levels. For example, if `sex_male` takes on the value 1, this indicates that the subscriber is a *male*. There is no `sex_female` dummy variable since if `sex_male` takes on the value 0, we know that the subscriber is *female* (since all of the subscribers in our dataset identified as either male or female). Now we are ready to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_uIVtOFjTG0"
   },
   "source": [
    "# Modeling our Data\n",
    "\n",
    "There are two possible objectives when constructing models for data -- modeling could be an inferential task where we seek to uncover and potentially understand relationships between predictors and a response (see MAT300 and QSO260) or modeling can be a task with a purely predictive goal (QSO370 and, to some extent, MAT434). The types of models built for both purposes overlap, but the metrics we use to evaluate and tweak those models change depending on the task. \n",
    "+ In **descriptive modeling** we seek to identify models that only include statistically significant predictors of a response and which explain a large portion of the variation in the response. That is, we seek models in which all predictors are accompanied by small $p$-values and a satisfactory adjusted $R^2$ value. Once we have this, we spend significant effort to better understand how a response might change if we allow one predictor to vary slightly while keeping all of the others constant. We might *use these models to decide how to spend capital (money, physical resources, efforts and manpower) to achieve some desired improvement in the response*.\n",
    "  + Descriptive models predict an *average* (expected) response value for all potential observations satisfying common features (ie. average yearly spending at a particular sporting goods store for all married, male customers, aged 30 - 45, with at least one child, and having a pre-tax income between \\\\$60000 and \\\\$80000 per year).\n",
    "+ In **predictive modeling** we build models whose only purpose is to predict accurately. We do not typically care to understand the relationship between predictors and response, nor do we often care what the model looks like or why it works. Our only concern is that it performs well.\n",
    "  + Predictive models are models doing a good job at predicting the value of a response for an *individual observation* (ie. next year's spending for Mike, who is a 36 year old married man with two children and who earns a salary of \\\\$76000 per year).\n",
    "\n",
    "In python we typically <u>use the `statsmodels` library for descriptive modeling</u> because it is very easy to print out all of the classical diagnostics (model coefficients, standard error estimates, $p$-values, residual standard error, $R^2$ and adjusted-$R^2$ values, etc.) for regression models. For <u>predictive modeling we prefer `sklearn`</u> (*scikit learn*) because it computes effective metrics for scoring predictive models by default (mean error, mean absolute error, root mean square error, mean percentage error, etc.). Since we care little about model coefficients here and more about model performace, `sklearn` output is more relevant and less distracting.\n",
    "\n",
    "We build models with both libraries below in order to show the difference.\n",
    "\n",
    "## Descriptive Models\n",
    "\n",
    "We start by building, reducing, and interpreting a descriptive model to predict average `charges` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7rp_eGZdGfp"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "all_columns = \"+\".join(train.loc[ :, train.columns != \"charges\"].columns)\n",
    "#The line above is shorthand for typing out \"ColName1 + ColName2 + ... + ColNameN\"\n",
    "#Run: print(all_columns) \n",
    "#to see this.\n",
    "\n",
    "my_formula = \"charges ~\" + all_columns\n",
    "#The formula for the model we wish to build is \"charges as explained by the sum\n",
    "#of terms containing our individual predictor variables\"\n",
    "lrMod = sm.formula.ols(my_formula, data = train)\n",
    "lrMod_fitted = lrMod.fit()\n",
    "#The two lines above run the regression procedure to fit the model.\n",
    "\n",
    "#Print model summary\n",
    "print(lrMod_fitted.summary()) \n",
    "\n",
    "#Compute and print RMSE (training error)\n",
    "print(\"RMSE (training error): \", math.sqrt(((train[\"charges\"] - lrMod_fitted.predict(train))**2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qRwqbZND9u2"
   },
   "source": [
    "The statistical output above gives lots of great information. In particular we have model metrics like adjusted R-Squared (closer to 1 is better), and information about individual terms (estimated coefficients, a p-value in the `P>|t|` column, and a confidence interval estimate for each model coefficient -- you might note that n cases where the p-value exceeds 0.05, a plausible coefficient of 0 falls within the confidence interval, suggesting that we do not have evidence to suggest that the corresponding predictor is significant -- it should be dropped).\n",
    "\n",
    "In descriptive modeling we would now identify insignificant predictors and drop them from the model, one at a time, removing the predictor with the highest p-value first. One note here is to remember that the dummy variables resulting from our *one-hot* encodings actually correspond to a single predictor -- if you wish to remove one of these dummies, you should remove them all. I've done that and will show the final resulting model below so that we can interpret it. You should try reducing the model yourself to ensure that you understand how this *backward elimination* procedure works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghZH1J2pFH6x"
   },
   "outputs": [],
   "source": [
    "#The *.difference() method can be used to compute the \"set-difference\". This\n",
    "#works as follows...if A = [\"a\", \"b\", \"c\"] and B = [\"a\", \"e\", \"i\", \"o\", \"u\"],\n",
    "#then,\n",
    "#A.difference(B)\n",
    "#will return [\"b\", \"c\"].\n",
    "#We can use this to remove columns from consideration in the regression model.\n",
    "all_columns = \"+\".join(train.loc[ :, train.columns.difference([\"charges\", \"hasChildren_oneToThree\", \"hasChildren_moreThanThree\", \"sex_male\", \"region_southwest\", \"region_northwest\", \"region_southeast\"])].columns)\n",
    "#The line above is shorthand for typing out \"ColName1 + ColName2 + ... + ColNameN\"\n",
    "#Run: print(all_columns) \n",
    "#to see this.\n",
    "\n",
    "my_formula = \"charges ~\" + all_columns\n",
    "#The formula for the model we wish to build is \"charges as explained by the sum\n",
    "#of terms containing our individual predictor variables\"\n",
    "lrMod = sm.formula.ols(my_formula, data = train)\n",
    "lrMod_fitted = lrMod.fit()\n",
    "#The two lines above run the regression procedure to fit the model.\n",
    "\n",
    "#Print model summary\n",
    "print(lrMod_fitted.summary()) \n",
    "\n",
    "#Compute and print RMSE (training error)\n",
    "print(\"RMSE (training error): \", math.sqrt(((train[\"charges\"] - lrMod_fitted.predict(train))**2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xciji09IHkKK"
   },
   "source": [
    "Now that we have a *final model*, we can interpret it. The resulting descriptive model we discovered is estimated as:\n",
    "$$\\mathbb{E}\\left[\\text{charges}\\right] = -12490 + 259.63*\\text{age} + 332.23*\\text{bmi} + 488.91*\\text{children} + 23720*\\text{smoker_yes}$$\n",
    "The adjusted R-squared value indicates that **our model explains about 75% of the variation in average insurance charges**. The coefficient on age indicates that, for all other variables held constant, a policy-holder who is one year older than the average policy holder will incur an expected additional \\$259.63 in yearly charges. See if you can interpret the coefficients on `children` and `bmi`. Similarly, all other variables held constant, a policy holder who is a smoker will incur an expected additional \\$23,720 in charges than a similar non-smoker (WOW!). The RMSE (*training error*) here is \\$6,129.41, which can be used to construct confidence and prediction intervals associated with expected charges. Remember though that this *training error* is typically an underestimate of true prediction error, so we will be more interested in the error estimated using our *test* and *safe* sets.\n",
    "\n",
    "## Predictive Linear Models\n",
    "\n",
    "Now that we've explored the construction and interpretation of a *descriptive model* using `statsmodels`, we will work to construct a *predictive model* using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUZd_bVRhy8a"
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "#Print the coefficients\n",
    "print(\"Intercept: \", lin_reg.intercept_)\n",
    "print(pd.DataFrame({\"Predictor\" : X_train.columns, \"Coefficient\" : lin_reg.coef_}))\n",
    "\n",
    "#Compute performance metrics\n",
    "meanErrorTrain = (y_train - lin_reg.predict(X_train)).mean()\n",
    "meanAbsoluteErrorTrain = (abs(y_train - lin_reg.predict(X_train))).mean()\n",
    "trainRMSE = (((y_train - lin_reg.predict(X_train))**2).mean())**0.5\n",
    "\n",
    "meanErrorTest = (y_test - lin_reg.predict(X_test)).mean()\n",
    "meanAbsoluteErrorTest = (abs(y_test - lin_reg.predict(X_test))).mean()\n",
    "testRMSE = (((y_test - lin_reg.predict(X_test))**2).mean())**0.5\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out training error metrics\n",
    "print(\"Training Error Metrics\")\n",
    "print(\"\\t The mean error on the training set is: $\", meanErrorTrain, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTrain, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", trainRMSE, sep = \"\")\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out test error metrics\n",
    "print(\"Test Error Metrics\")\n",
    "print(\"\\t The mean error on the test set is: $\", meanError, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteError, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", testRMSE, sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtWuc9W2kUzQ"
   },
   "source": [
    "You'll notice that the coefficients for both original models agree (look back at our **first** model from the descriptive section) but that the metrics we are interested in differ in each case. With stats models we get the more classical output and diagnostics regarding fit. With `sklearn` we can access all of this same information, but it is not available in a format for convenient printing. There is much more information contained in the stat models regression summary output but most of it is extraneous for predictive purposes.\n",
    "\n",
    "Notice further that we computed error metrics twice -- once on the training data and once on the test data. We expect the test metrics to be slightly worse than the training metrics since the test data was unknown to the model. The test data simulates new data and so the error metrics resulting from the test set are more likely to reflect what we should expect to see in model deployment. Here the $\\tt{MAE}$ and $\\tt{RMSE}$ metrics are both slightly better on the test data than on the training data. This should indicate that our model is *underfitting* the training data. We should have expected our model to underfit here though. There are clearly relationships between charges and our features which we did not model well. For example, we assumed a \"straight-line\" relationship between `age` and `charges`, where there is clearly some curvature in that relationship. Additionally, we saw evidence to suggest that `age` and `bmi` may not influence `charges` independently of one another -- there is evidence here that we should consider the possibility for interaction. We'll see how to implement these things next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZpPSakkK_70"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Now you know how to build, display, and interpret/analyze regression models for both *descriptive* and *predictive* puroses. You should test your new-found knowledge by building regression models with a dataset that you find interesting, perhaps you could use our BikeShare competition data set.\n",
    "\n",
    "See you next week, for some advanced techniques for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGIM0ptM30SB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "9_Regression_PartI.ipynb",
   "provenance": [
    {
     "file_id": "1b82w7y7JW2qq3zMQ0X9xs1xWf1GXstyR",
     "timestamp": 1578321565295
    },
    {
     "file_id": "1xedbseYti1b2aaeMUb7YsyOtny8I6oPF",
     "timestamp": 1573156909581
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
