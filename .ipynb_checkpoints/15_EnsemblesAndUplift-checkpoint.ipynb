{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "credit = pd.read_csv(\"http://faculty.marshall.usc.edu/gareth-james/ISL/Credit.csv\")\n",
    "credit.drop([\"Unnamed: 0\"], axis = 1, inplace = True)\n",
    "hair = pd.read_csv(\"C:/Users/agilb/Desktop/Datasets/hairUplift.csv\")\n",
    "hair.dropna(axis = 0, inplace = True)\n",
    "hair.drop([\"Validation\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Models: Ensembles and Uplift\n",
    "\n",
    "In this notebook we consider the possibility of mixing multiple models to create a *super-model* which performs better than the individual models it is built from. For example, why not combine a decision tree with a series of regression models to predict a numerical response? Now we're really cooking with gas!\n",
    "\n",
    "## A reminder of the model classes we've covered\n",
    "\n",
    "As a reminder, we've covered two classes of models for regression applications as well as two classes of model for classification. \n",
    "+ **Regression**\n",
    "    + Linear Regression Models\n",
    "    + Regression Trees\n",
    "+ **Classification**\n",
    "    + Classification Trees\n",
    "    + Logistic Regression Models\n",
    "\n",
    "There are many more classes of model which exist -- some of them are in your textbook -- one day I hope you'll read all of those chapters you didn't read as part of this course. There are even more models that our book does not consider -- new model classes are being discovered at a rapid pace.\n",
    "\n",
    "In this notebook (and in Chapter 13) of our book, we discuss how one might mix multiple models together in an effort to make superior predictions than a single model choice could produce. The idea is this -- different models perform poorly on different types of observations, so if we can somehow aggregate predictions together the good predictions will remain good while the bad predictions become somewhat improved.\n",
    "\n",
    "## Several strategies\n",
    "\n",
    "There are several strategies for ensembling a set of models. Some require a homogeneous set of models (a set in which every model is a linear regressor or every model is a classification tree, etc.) and others which benefit from a heterogeneous set of models (some linear regressors, some regression trees, etc.). We will look at just a few different strategies here, but more are listed below.\n",
    "+ *Bootstrap Aggregation (Bagging)* treats sample data as if it were a population -- pulling hundreds or thousands of new \"samples\" from the training data and fitting a model to each individual bootstrapped sample. Now we end up with hundreds or thousands of models from which we can aggregate predictions.\n",
    "+ There are two common *boosting* algorithms -- adaptive boosting (AdaBoost) and gradient boosting. \n",
    "    + Adaptive boosting fits a model on the data, identifies observations which the model gave poor predictions for, increases the \"weight\" of those observations and fits a second model, again indentifies the observations which had poor predictions, increases the \"weight\" of those and fits a third model, continuing until a stopping point is reached. \n",
    "    + Gradient boosting fits a sequence of *weak learners* to the training data. A weak learner is a model which does only slightly better than random guessing. A weak learner is fit to the training data and then residuals are computed, a second weak learner is fit to predict the residuals and then new residuals are computed (residuals of residuals -- sort of like the *Inception* movie), and we continue until reaching a threshold at which improvements made are too small to continue. The idea here is that the weak learners slowly chip away at the reducible error, and since learning occurs slowly we are unlikely to overfit the training data.\n",
    "+ *Stacking* refers to the act of building several predictive models and then building a new predictive model which takes their outputs as inputs before arriving at a prediction. This can be done with as many stacked layers as we would like. In fact, the research team which won the famous \"Netflix Prize\" used a stacked ensemble -- just one of their layers included over 30 models!\n",
    "+ While all of these other techniques maintain a goal of improving predictive performance, the *uplift* technique has a completely different objective. Uplift models A/B testing to determine which observations would respond most positively to an outreach effort. This is a common technique in marketing and also in politics -- we are interested in knowing which customers will be swayed to purchase a product if sent a coupon (but we don't want to send a coupon to someone who was going to buy the product anyway). \n",
    "\n",
    "There is so much interesting work happening in the area of ensembles. Since we have just a single week, we will focus only on *bagging* and *uplift*. We'll start by exploring bootstrap aggregation by working with a relatively small dataset on the credit ratings of 400 individuals. Our goal will be to predict the credit rating for these individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have several categorical columns we'll use one-hot-encoding now, before we split into *training*, *test*, and *safe* sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.get_dummies(credit, columns = [\"Gender\", \"Student\", \"Married\", \"Ethnicity\"], drop_first = True)\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into our ensemble methods, let's split the data into *training*, *test*, and *safe* sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit.drop([\"Rating\"], axis = 1)\n",
    "y = credit[\"Rating\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "X_safe, X_test, y_safe, y_test = train_test_split(X_temp, y_temp, test_size = 0.6, random_state = 42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Recall that in bagging, we treat the training data as if it were the population and draw many samples from that dataset, fitting a model on each of the random samples, making predictions from each model and then aggregating them. In regression applications it is natural to perform aggregation via some measure of central tendency (the *mean* or *median*), while for classification applications we opt for *majority vote* if no propensities are computed, but may opt to either use an average propensity metric or a majority vote if propensities are being computed by our models. Since our application is a regression scenario, we'll take the mean of all the predicted responses as the ultimate prediction for credit `Rating` for each individual.\n",
    "\n",
    "As a reminder, bagging occurs in several stages. \n",
    "1. We pull a random sample from the *training* data. Typically this random sample is the same size as the training set and  allows for repeated selection of rows. This stage is what is known as *bootstrapping*.\n",
    "2. We fit a model on each of these bootstrapped data sets.\n",
    "3. We use each individual model to make a prediction on each of the test observations.\n",
    "4. Once we have these predictions, we aggregate them via some aggregation function. Here we will take the usual *mean* of the predictions.\n",
    "\n",
    "We run through the steps below, for 1000 bootstrapped samples, but you can change this by editing the value assigned to `n_boot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boot = 1000\n",
    "\n",
    "preds = pd.DataFrame()\n",
    "\n",
    "for i in list(range(n_boot)):\n",
    "    tree_reg = DecisionTreeRegressor()\n",
    "    \n",
    "    indices = np.random.choice(len(X_train), size = len(X_train), replace = True)\n",
    "    boot_samp = X_train.iloc[indices, :]\n",
    "    boot_responses = y_train.iloc[indices]\n",
    "    \n",
    "    tree_reg.fit(boot_samp, boot_responses)\n",
    "    \n",
    "    preds[\"preds\" + str(i)] = tree_reg.predict(X_test)\n",
    "\n",
    "boot_preds = preds.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line of the code block above takes the 1000 predictions for each of our 60 test observations, and averages them to make one final prediction. Let's see how good we did -- since this is a regression application we will be computing the $\\tt{RMSE}$. This first code block below resets the index on `y_test`. We need to do this since the `index` provides an extra \"check\" when adding or subtracting columns of data frames -- if the indices don't match, Python won't let us carry out the operation in the way we intend to. The second code block computes the $\\tt{RMSE}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRMSE = (((y_test - boot_preds)**2).mean())**0.5\n",
    "print(\"The bootsrap RMSE is: \", testRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing our bootsrap RMSE to the test RMSE from just a single linear regressor, we should see a minor improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "(((y_test - tree_reg.predict(X_test))**2).mean())**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we see that boostrapping results in a significant reduction of the expected prediction error. It turns out that linear regression is better-suited to this credit score application than trees are. See if you can adapt our code to *bagg* an ensemble of linear regression models -- how much improvement does that ensemble give over our trees? It is important to remember that even seemingly minor iprovements on an individual level may lead to significant impacts in performance when an entire customer base is considered.\n",
    "\n",
    "To recap, we created a set of 1000 bootstrapped samples, fit a model on each sample, used each model to predict the credit rating for an individual in our test set, aggregated those predictions by taking the mean prediction as the predicted response for each test observation, and saw an improvement in performance over just constructing a single linear regressor on the training data and applying it to the test data. Good work. Now we'll move on to exploring *uplift*.\n",
    "\n",
    "### Uplift\n",
    "\n",
    "As a reminder, uplift is a technique which can aid in targeted marketing campaigns. We'll consider a dataset containing 10,000 records of customers, whether or not they received a promotional advertisement, and whether or not they purchased a particular hair product. The dataset also includes a few additional characteristics of each customer. Our goal will be to better understand the type of customers who are most positively swayed by a promotion so that we can target these customers with a marketing campaign.\n",
    "\n",
    "In this application, an A/B test has already been conducted. A random subset of the 10,000 customers received a promotion (`Promotion_ord`) while the remaining customers received no promotion. We then recorded which customers went forward with making the `Purchase`. It is important to note, however, that we do not have data on whether customers intended to buy the product prior to receiving the outreach. We'll use the *test* observations to approximate uplift by making a prediction on each test observation assuming no promotion was sent, and a separate prediction assuming that a promotion was indeed sent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hair.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by using one-hot-encoding on the `Hair Color` and `U.S. Region` variables. Then we'll split the data into a *training* and *test* set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hair = pd.get_dummies(hair, columns = [\"Hair Color\", \"U.S. Region\"], drop_first = True)\n",
    "\n",
    "X = hair.drop([\"Purchase\"], axis = 1)\n",
    "y = hair[\"Purchase\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been pre-processed and split, let's start building an uplift model. We'll work with a logistic regression classifier since we know that it natively outputs a *propensity*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "log_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fit the logistic regressor, we'll use it to predict the *propensity* for each *test* customer to purchase under the assumption that they don't receive a promotion, and also the propensity for each test customer to purchase under the assumption that they do receive a promotion. Notice that we use the `.predict_proba()` method rather than the `.predict()` method since `.predict_proba()` results in a propensity rather than a direct class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uplift = X_test.copy()\n",
    "\n",
    "X_test_promotion = X_test.copy()\n",
    "X_test_promotion[\"Promotion_ord\"] = 1\n",
    "\n",
    "X_uplift[\"Propensity_Promotion\"] = log_clf.predict_proba(X_test_promotion)[:, 1]\n",
    "\n",
    "X_test_no_promotion = X_test.copy()\n",
    "X_test_no_promotion[\"Promotion_ord\"] = 0\n",
    "\n",
    "X_uplift[\"Propensity_NoPromotion\"] = log_clf.predict_proba(X_test_no_promotion)[:, 1]\n",
    "X_uplift.head()\n",
    "\n",
    "X_uplift[\"Uplift\"] = X_uplift[\"Propensity_Promotion\"] - X_uplift[\"Propensity_NoPromotion\"]\n",
    "X_uplift.sort_values(\"Uplift\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the uplift values we are seeing are not extremely high -- the record with the greatest uplift is still just about 3% more likely to purchase after receiving the promotion. If you check the propensity to purchase without the promotion, however, these propensities are very small too. So while a 3% boost in the likelihood of purchase sounds small, for our top flagged customers, a mailer is expected to double the likelihood that they purchase the product. Such a mailer might be a good investment.\n",
    "\n",
    "## Summary\n",
    "\n",
    "There you have it, we saw two new applications of model building this week. We discussed ensembles of models and stacking. There are lots of methods for constructing ensembles -- while we only implemented one (*bootstrap aggregation*), we mentioned several -- you should look more deeply into those techniques. There are lots of tutorials available on the web. \n",
    "\n",
    "In the second half of this notebook we considered a technique called uplift modeling. Rather than a predictive modeling technique, this method looks at flagging customers who we think would be most positively impacted by a promotional offer. This technique is really popular in marketing and could be applied to our earlier example of customer churn. In that case, we would be looking for large negative values for uplift since our goal is to make it *less* likely that a customer would churn.\n",
    "\n",
    "## About the remainder of our course\n",
    "\n",
    "As a reminder, the remainder of our course contains three modules.\n",
    "+ The *unsupervised learning* module is great for those of you who may be interested in customer segmentation, market-basket analysis, and recommendation engines.\n",
    "+ The *time series* module is applicable to those of you who may be most interested in forecasting.\n",
    "+ The *unstructured data* module covers some truly cutting-edge techniques in social media monitoring and text analysis. If you are interested in building algorithms that can monitor social media conversations on a topic or company, or to quickly synthesis thousands of product revews, then this is a module for you.\n",
    "\n",
    "As a reminder, undergraduates must complete at least one of these modules while graduate students must complete two. Each module contains a prompt for a significant final project which each student must complete. Graduate students should choose only one of the final projects to complete -- you are not responsible for two large-scale projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
