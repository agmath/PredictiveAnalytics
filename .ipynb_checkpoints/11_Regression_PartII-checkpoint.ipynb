{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3J3UMwkFk4Y"
   },
   "source": [
    "# Multiple Linear Regression (Part II): Advanced Techniques for Predicting Insurance Premiums\n",
    "\n",
    "This jupyter notebook is a continuation of our discussion on regression models in both the descriptive and predictive settings. This notebook and the corresponding assignments (see BrightSpace) should take you about a week to work through -- please contact me with questions you have. The first part of this set of notebooks covered an exploratory analysis as well as the construction of a linear regression model to predict the `charges` incurred by an insurance policyholder. In this second notebook, we consider methods which can be used to improve our model's performance. \n",
    "\n",
    "The next section of this notebook concerns a few reminders about our data, followed by the necessary code to get back to where we were at the end of last week's notebook.\n",
    "\n",
    "## Reminders\n",
    "\n",
    "### About our data\n",
    "\n",
    "- Our simple dataset contains a few attributes for each person such as \n",
    "- Age, Sex, BMI, Children, Smoker, Region and their charges\n",
    "\n",
    "### Goal\n",
    "- To use this info to predict charges for new customers\n",
    "  - An insurance company can use this information to inform *rate-setting*\n",
    "  \n",
    "## Preliminaries\n",
    "\n",
    "The code that follows is the code necessary to reproduce the *training*, *test*, and *safe* sets as well as the predictive model we built at the close of last week's Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "executionInfo": {
     "elapsed": 2068,
     "status": "ok",
     "timestamp": 1603197560637,
     "user": {
      "displayName": "Adam Gilbert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj1KWqLKG2SAQxx8KvqzqhM94EvqxkXs2iOw7gQWQ=s64",
      "userId": "02074648007699947871"
     },
     "user_tz": 240
    },
    "id": "m3xySfs8Fjj7",
    "outputId": "0e13a655-5fcc-4fe2-ddd5-12a3df059412"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "file_name = \"https://raw.githubusercontent.com/rajeevratan84/datascienceforbusiness/master/insurance.csv\"\n",
    "insurance = pd.read_csv(file_name)\n",
    "\n",
    "#Feature Engineering and Cleaning\n",
    "insurance[\"hasChildren\"] = 0*insurance.size\n",
    "##Populate the new column with the appropriate values\n",
    "insurance[\"hasChildren\"].loc[insurance[\"children\"] == 0] = \"None\"\n",
    "insurance[\"hasChildren\"].loc[(insurance[\"children\"] > 0) & (insurance[\"children\"] < 4)] = \"oneToThree\"\n",
    "insurance[\"hasChildren\"].loc[insurance[\"children\"] >= 4] = \"moreThanThree\"\n",
    "\n",
    "insurance = pd.get_dummies(insurance, columns = [\"hasChildren\", \"sex\", \"smoker\", \"region\"], drop_first= True)\n",
    "insurance.drop([\"children\"], axis = 1, inplace = True)\n",
    "#Re-split to training, test, and safe data\n",
    "X = insurance.drop([\"charges\"], axis = 1)\n",
    "y = insurance[\"charges\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "X_safe, X_test, y_safe, y_test = train_test_split(X_temp, y_temp, test_size = 0.6, random_state = 42)\n",
    "\n",
    "#Instantiate, fit, and validate regressor\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "#Print the coefficients\n",
    "print(\"Intercept: \", lin_reg.intercept_)\n",
    "print(pd.DataFrame({\"Predictor\" : X_train.columns, \"Coefficient\" : lin_reg.coef_}))\n",
    "\n",
    "#Compute performance metrics\n",
    "meanErrorTrain = (y_train - lin_reg.predict(X_train)).mean()\n",
    "meanAbsoluteErrorTrain = (abs(y_train - lin_reg.predict(X_train))).mean()\n",
    "trainRMSE = (((y_train - lin_reg.predict(X_train))**2).mean())**0.5\n",
    "\n",
    "meanErrorTest = (y_test - lin_reg.predict(X_test)).mean()\n",
    "meanAbsoluteErrorTest = (abs(y_test - lin_reg.predict(X_test))).mean()\n",
    "testRMSE = (((y_test - lin_reg.predict(X_test))**2).mean())**0.5\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out training error metrics\n",
    "print(\"Training Error Metrics\")\n",
    "print(\"\\t The mean error on the training set is: $\", meanErrorTrain, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTrain, sep = \"\")\n",
    "print(\"\\t The training RMSE is: $\", trainRMSE, sep = \"\")\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out test error metrics\n",
    "print(\"Test Error Metrics\")\n",
    "print(\"\\t The mean error on the test set is: $\", meanErrorTest, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTest, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", testRMSE, sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtWuc9W2kUzQ"
   },
   "source": [
    "As we mentioned at the end of the last notebook, we should have expected our model to underfit. While we assumed a \"straight-line\" relationship between `age` and `charges`, where there is clearly some curvature in that relationship -- open last week's notebook to see the evidence in the data visualizations we created. We also mentioned visual evidence to suggest that `age` and `bmi` may not influence `charges` independently of one another -- there is evidence here that we should consider the possibility for interaction. \n",
    "\n",
    "## Searching for Improvements\n",
    "\n",
    "Now that we are back to where we left off, let's see what improvements we can make. Improvements can come in multiple forms -- we'll consider the items below.\n",
    "\n",
    "+ Adjustments to the model\n",
    "    + Consider additional predictors -- these could be predictors that were initially excluded from the model fitting, including relevant features from an external source, or engaging in more feature engineering.\n",
    "    + Consider *standardizing* the model features. In particular, if we have features on drastically different scales (age in years, and yearly salary in dollars), our model may suffer. We can sometimes see improvements by putting all of our numerical predictors on a common scale -- say one with mean 0 and standard deviation 1.\n",
    "    + Consider *higher-order terms*\n",
    "        + Maybe we want to allow for curvature -- this is achieved by allowing the model to include terms which raise features to powers (squaring, cubing, etc).\n",
    "        + Maybe we want to allow for pairs of features to influence the response in a way that is not independent of one-another -- we do this by engineering new features which combine existing features in some way (multilying them together, taking a quotient, etc).\n",
    "    + Utilize *regularization* to prevent your model from overfitting the training data.\n",
    "    + Consider a different model class altogether -- while we won't do this here, it is not uncommon to build and test several competing model classes during a project (linear regressors, decision tree regressors, neural networks, etc.) and see which one performs the best.\n",
    "+ Improve our estimate of the true prediction error.\n",
    "    + In our explorations so far we've used the validation-set approach to estimate our true prediction error. We say that the prediction error is approximated by the test error. While this is true, it is not necessarily well-approximated by the test error. Our error metric is dependent on the test set that was randomly chosen -- it is possible that we got a \"really easy\" or \"really difficult\" *test* set, and so the test error is not a good indicator of true prediction error. \n",
    "        + In order to see this in action, re-run the code block above a few times, changing the value assigned to the `random_state` parameter in `train_test_split` each time. Do the test error metrics remain constant?\n",
    "        + One way to have more confidence in our estimate for the true prediction error is to compute many different test errors, and then average them. We'll use a technique called cross-validation for that purpose in this notebook. Another advantage to cross-validation over the simple validation-set approach is that cross-validation can help us with model-selection!\n",
    "\n",
    "Okay, that's a lot of stuff to tell you about without actually showing you what any of it really means. Let's start employing these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thBpQtAk6_gw"
   },
   "source": [
    "## Scaling (standardizing) Features\n",
    "\n",
    "In predictive modeling it can be a real advantage to scale all of your numerical predictors so that they are on a common scale. Typically we standardize so that each predictor has a mean of 0 and a standard deviation of 1, but also somewhat common is min-max scaling to the [0, 1] interval. There are several advantages here:\n",
    "+ We no longer need to estimate an intercept which takes away some of the uncertainty in modeling\n",
    "+ Some model classes actually require the normalized predictors in order to be effective. In particular, distance-based models like K Nearest Neighbors (KNN) fail if features are not on a common scale.\n",
    "\n",
    "In machine learning applications (ie. predictive modeling) we typically don't care to interpret our models, so the slight loss in interpretive value that comes with this scaling is not a concern. In predictive tasks I would always recommend standardizing all of your numerical predictors. We can see how such scaling works below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KW74kZPOWTF0"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ulfn5N-ifPg"
   },
   "source": [
    "Scaling is not meaningful for categorical variables (or their corresponding dummy variables), so we'll only scale our numeric columns (`age` and `bmi`). We can perform standardization with a `StandardScaler` object from the `sklearn.preprocessing` library. Remember that it is good practice to load all of your required libraries at the beginning of your Jupyter notebook rather than loading them \"as needed\" throughout. I'm loading the libraries as they are needed here just to make which libraries are required for each task more explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDZPJXxMdGfy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train_num = X_train.iloc[:, :2]\n",
    "X_test_num = X_test.iloc[:, :2]\n",
    "X_safe_num = X_safe.iloc[:, :2]\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train_num)\n",
    "X_train_num_scaled = sc.transform(X_train_num)\n",
    "X_test_num_scaled = sc.transform(X_test_num)\n",
    "X_safe_num_scaled = sc.transform(X_safe_num)\n",
    "\n",
    "X_train_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNbNTOvLifPm"
   },
   "source": [
    "Okay, in the code block above, we created three new data frames, `X_train_num`, `X_test_num`, and `X_safe_num`, each containing only the two numeric columns `age` and `bmi`. After doing that, we istantiate a `StandardScaler` object and fit *fit* it  on the *training* data. This computes the mean and standard deviation for each of our columns from the training observations. Then we *transform* the three data frames containing the numeric columns. Notice that we standardize the columns in all cases using the mean and standard deviation corresponding to the *training* set. Recall from introductory statistics that standardization is done by subtracting the mean and dividing by the standard deviation ($\\displaystyle{\\frac{x - \\mu_{\\text{train}}}{\\sigma_{\\text{train}}}}$). \n",
    "\n",
    "When we ask Python to print out the contents of `X_train_num_scaled` we see that the values have been transformed, but also that the structure is no longer a data frame -- it is a `numpy` array instead. We will now replace the original `age` and `bmi` columns in the three data frames with these new scaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cj7uO-UTifPm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training Data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[\"age_scaled\"] = X_train_num_scaled[:, 0]\n",
    "X_train_scaled[\"bmi_scaled\"] = X_train_num_scaled[:, 1]\n",
    "X_train_scaled.drop([\"age\", \"bmi\"], axis = 1, inplace = True)\n",
    "\n",
    "#Test Data\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[\"age_scaled\"] = X_test_num_scaled[:, 0]\n",
    "X_test_scaled[\"bmi_scaled\"] = X_test_num_scaled[:, 1]\n",
    "X_test_scaled.drop([\"age\", \"bmi\"], axis = 1, inplace = True)\n",
    "\n",
    "#Safe Data\n",
    "X_safe_scaled = X_safe.copy()\n",
    "X_safe_scaled[\"age_scaled\"] = X_safe_num_scaled[:, 0]\n",
    "X_safe_scaled[\"bmi_scaled\"] = X_safe_num_scaled[:, 1]\n",
    "X_safe_scaled.drop([\"age\", \"bmi\"], axis = 1, inplace = True)\n",
    "\n",
    "#Check to ensure we have what we expected\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPn-J6GOeDzB"
   },
   "source": [
    "Okay, everything seems to have worked out quite well. The `age` and `bmi` features have been replaced by `age_scaled` and `bmi_scaled` -- aside from these new columns being moved to the end of the dataframe, everything looks just about the same as it did before.\n",
    "\n",
    "## Fitting a Linear Model on Scaled Predictors\n",
    "\n",
    "There's nothing new here -- we fit our regression model the same way as we did earlier -- just now we have numerical predictors which are all standardized to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNzu6sCHdGf6"
   },
   "outputs": [],
   "source": [
    "scaled_reg = LinearRegression()  # Create a instance for Linear Regression model\n",
    "scaled_reg.fit(X_train_scaled, y_train)  # Fit data to the model\n",
    "\n",
    "#Print the coefficients\n",
    "print(\"Intercept: \", scaled_reg.intercept_)\n",
    "print(pd.DataFrame({\"Predictor\" : X_train_scaled.columns, \"Coefficient\" : scaled_reg.coef_}))\n",
    "\n",
    "#Compute performance metrics\n",
    "meanErrorTrainScaled = (y_train - scaled_reg.predict(X_train_scaled)).mean()\n",
    "meanAbsoluteErrorTrainScaled = (abs(y_train - scaled_reg.predict(X_train_scaled))).mean()\n",
    "trainRMSEscaled = (((y_train - scaled_reg.predict(X_train_scaled))**2).mean())**0.5\n",
    "\n",
    "meanErrorTestScaled = (y_test - scaled_reg.predict(X_test_scaled)).mean()\n",
    "meanAbsoluteErrorTestScaled = (abs(y_test - scaled_reg.predict(X_test_scaled))).mean()\n",
    "testRMSEscaled = (((y_test - scaled_reg.predict(X_test_scaled))**2).mean())**0.5\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out training error metrics\n",
    "print(\"Training Error Metrics\")\n",
    "print(\"\\t The mean error on the training set is: $\", meanErrorTrainScaled, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTrainScaled, sep = \"\")\n",
    "print(\"\\t The training RMSE is: $\", trainRMSEscaled, sep = \"\")\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out test error metrics\n",
    "print(\"Test Error Metrics\")\n",
    "print(\"\\t The mean error on the test set is: $\", meanErrorTestScaled, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTestScaled, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", testRMSEscaled, sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HHkKBNqg_YF"
   },
   "source": [
    "Notice that the training and test metrics are nearly identical to the metrics we observed in the un-scaled case. The reason we did not gain a benefit from scaling is because `age` and `bmi` were already on similar scales. Let's try something else!\n",
    "\n",
    "### Polynomial Features and Curvi-Linear Regression\n",
    "\n",
    "So far we've explored multiple linear regression in which all predictors are included in first-order terms. This assumes a straight line relationship between each predictor and the response. It is also possible to add curved relationships or interactions between predictors. We can do this by *engineering* higher-order features (say, a feature for `age`$^2$) or we can automate the process by using `sklearn`'s `PolynomialFeatures` method. There are pros and cons to both methods -- here we'll do the feature engineering \"by hand\" so that you can see what is being done \"under the hood\". You may also notice that I'm breaking my suggestion about doing feature engineering only once on the original dataset and then re-splitting the *training*, *test*, and *safe* sets -- good for noticing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDBy6QMxdGf-"
   },
   "outputs": [],
   "source": [
    "X_train_poly = X_train.copy()\n",
    "X_train_poly[\"age2\"] = (X_train_poly[\"age\"])**2\n",
    "X_train_poly[\"age_bmi\"] =(X_train_poly[\"age\"])*(X_train_poly[\"bmi\"])\n",
    "X_train_poly[\"bmi2\"] = (X_train_poly[\"bmi\"])**2\n",
    "X_train_poly[\"age3\"] = (X_train_poly[\"age\"])**3\n",
    "X_train_poly[\"age2_bmi\"] = (X_train_poly[\"age2\"])*(X_train_poly[\"bmi\"])\n",
    "X_train_poly[\"age_bmi2\"] = (X_train_poly[\"age\"])*(X_train_poly[\"bmi2\"])\n",
    "X_train_poly[\"bmi3\"] = (X_train_poly[\"bmi\"])**3\n",
    "\n",
    "X_test_poly = X_test.copy()\n",
    "X_test_poly[\"age2\"] = (X_test_poly[\"age\"])**2\n",
    "X_test_poly[\"age_bmi\"] =(X_test_poly[\"age\"])*(X_test_poly[\"bmi\"])\n",
    "X_test_poly[\"bmi2\"] = (X_test_poly[\"bmi\"])**2\n",
    "X_test_poly[\"age3\"] = (X_test_poly[\"age\"])**3\n",
    "X_test_poly[\"age2_bmi\"] = (X_test_poly[\"age2\"])*(X_test_poly[\"bmi\"])\n",
    "X_test_poly[\"age_bmi2\"] = (X_test_poly[\"age\"])*(X_test_poly[\"bmi2\"])\n",
    "X_test_poly[\"bmi3\"] = (X_test_poly[\"bmi\"])**3\n",
    "\n",
    "X_safe_poly = X_safe.copy()\n",
    "X_safe_poly[\"age2\"] = (X_safe_poly[\"age\"])**2\n",
    "X_safe_poly[\"age_bmi\"] =(X_safe_poly[\"age\"])*(X_safe_poly[\"bmi\"])\n",
    "X_safe_poly[\"bmi2\"] = (X_safe_poly[\"bmi\"])**2\n",
    "X_safe_poly[\"age3\"] = (X_safe_poly[\"age\"])**3\n",
    "X_safe_poly[\"age2_bmi\"] = (X_safe_poly[\"age2\"])*(X_safe_poly[\"bmi\"])\n",
    "X_safe_poly[\"age_bmi2\"] = (X_safe_poly[\"age\"])*(X_safe_poly[\"bmi2\"])\n",
    "X_safe_poly[\"bmi3\"] = (X_safe_poly[\"bmi\"])**3\n",
    "\n",
    "polynomial_reg = LinearRegression()  # Create a instance for Linear Regression model\n",
    "polynomial_reg.fit(X_train_poly, y_train)  # Fit data to the model\n",
    "\n",
    "#Compute performance metrics\n",
    "meanErrorTrainPoly = (y_train - polynomial_reg.predict(X_train_poly)).mean()\n",
    "meanAbsoluteErrorTrainPoly = (abs(y_train - polynomial_reg.predict(X_train_poly))).mean()\n",
    "trainRMSEpoly = (((y_train - polynomial_reg.predict(X_train_poly))**2).mean())**0.5\n",
    "\n",
    "meanErrorTestPoly = (y_test - polynomial_reg.predict(X_test_poly)).mean()\n",
    "meanAbsoluteErrorTestPoly = (abs(y_test - polynomial_reg.predict(X_test_poly))).mean()\n",
    "testRMSEpoly = (((y_test - polynomial_reg.predict(X_test_poly))**2).mean())**0.5\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out training error metrics\n",
    "print(\"Training Error Metrics\")\n",
    "print(\"\\t The mean error on the training set is: $\", meanErrorTrainPoly, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTrainPoly, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", trainRMSEpoly, sep = \"\")\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out test error metrics\n",
    "print(\"Test Error Metrics\")\n",
    "print(\"\\t The mean error on the test set is: $\", meanErrorTestPoly, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTestPoly, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", testRMSEpoly, sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeSHROgGjfYa"
   },
   "source": [
    "We saw an improvement in the training error but an increase in the test error. This model is overfit. We had two major problems here: 1) we extended polynomial terms for all of our predictors, where we really only have evidence that the `age` variable has a curved relationship with `charges` and 2) we jumped to a 3rd degree polynomial which is very flexible. Let's build an intermediate model which includes only a term for `age`$^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aS7j-_df7oML"
   },
   "outputs": [],
   "source": [
    "#Create an age^2 column in our data frame\n",
    "X_train_quadAge = X_train.copy()\n",
    "X_train_quadAge[\"age2\"] = X_train_quadAge[\"age\"]**2\n",
    "X_test_quadAge = X_test.copy()\n",
    "X_test_quadAge[\"age2\"] = X_test_quadAge[\"age\"]**2\n",
    "X_safe_quadAge = X_safe.copy()\n",
    "X_safe_quadAge[\"age2\"] = X_safe_quadAge[\"age\"]**2\n",
    "\n",
    "age2_reg = LinearRegression()\n",
    "age2_reg.fit(X_train_quadAge, y_train)\n",
    "\n",
    "#Compute performance metrics\n",
    "meanErrorTrainQuad = (y_train - age2_reg.predict(X_train_quadAge)).mean()\n",
    "meanAbsoluteErrorTrainQuad = (abs(y_train - age2_reg.predict(X_train_quadAge))).mean()\n",
    "trainRMSEquad = (((y_train - age2_reg.predict(X_train_quadAge))**2).mean())**0.5\n",
    "\n",
    "meanErrorTestQuad = (y_test - age2_reg.predict(X_test_quadAge)).mean()\n",
    "meanAbsoluteErrorTestQuad = (abs(y_test - age2_reg.predict(X_test_quadAge))).mean()\n",
    "testRMSEquad = (((y_test - age2_reg.predict(X_test_quadAge))**2).mean())**0.5\n",
    "\n",
    "#Print out training error metrics\n",
    "print(\"Training Error Metrics\")\n",
    "print(\"\\t The mean error on the training set is: $\", meanErrorTrainQuad, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTrainQuad, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", trainRMSEquad, sep = \"\")\n",
    "\n",
    "#Print out an empty line to make output more readable\n",
    "print(\"\\n\")\n",
    "\n",
    "#Print out test error metrics\n",
    "print(\"Test Error Metrics\")\n",
    "print(\"\\t The mean error on the test set is: $\", meanErrorTestQuad, sep = \"\")\n",
    "print(\"\\t The mean absolute error on the test set is: $\", meanAbsoluteErrorTestQuad, sep = \"\")\n",
    "print(\"\\t The test RMSE is: $\", testRMSEquad, sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv7uyEK47aua"
   },
   "source": [
    "Good -- both the training and test errors dropped slightly from our original model. This gives us evidence to suggest that the relationship between `age` and `charges` is curved, as we suspected. \n",
    "\n",
    "We can use a new model-fitting process called *Cross-Validation* to obtain better (more stable) estimates for our true prediction error as well as to help us choose the appropriate level of flexibility for our model.\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "Cross validation is an advanced version of the validation set approach (training and test set). Rather than break off a specific test set, we break our data into $k$ folds. Each fold takes a turn being left out from model building procedures and behaves as a test set. This means that if we have 10 folds, we fit a particular model 10 times and get 10 different test error estimates. We can then average these test error estimates to obtain our *cross validation error*. The cross validation error is typically a more reliable error estimate than the test error is (due to the law of averages), and we get the additional advantage of being able to compute standard error in this error estimate.\n",
    "\n",
    "We'll start by joining the *training* and *test* data back into a single set. We can do this since cross-validation will create folds which will be used as test sets during the cross-validation process. Having a separate test set left out is redundant. Remember though, we still have our *safe* data which has not been opened yet -- it will be used as a final test for our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwrlHUnBifQC"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "X_trainCV = pd.concat([X_train, X_test])\n",
    "y_trainCV = pd.concat([y_train, y_test])\n",
    "print(X_trainCV.shape)\n",
    "X_trainCV.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QChVhsPifQG"
   },
   "source": [
    "We can see from the `.shape` printouts, the new data frame `X_trainCV` does contain all of the rows from `X_train` and all of the rows from `X_test` as well. Now we move forward to implementing cross-validation. We will do cross-validation with 10 folds (set from the `cv = 10` argument) which means that we will be fitting our linear regression model 10 times, and computing 10 error estimates. The models will be scored according to the *negative root mean squared error*, this is because the *scoring* function must be a function which is to be maximized. The smallest $\\tt{RMSE}$ will correspond to the largest negative $\\tt{RMSE}$.\n",
    "\n",
    "Since we know that cross-validation can help us with model-selection, we'll re-run most of our models with the cross-validation method. We omit the model in which we used the standardized `age` and `bmi` features since we saw no change between using the scaled versus original features previously. We'll start with our original model -- the straight-line linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3xwOOgCdGgS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_validate, cross_val_score  # For K-Fold Cross Validation\n",
    "#from sklearn.metrics import r2_score  # For find accuracy with R2 Score\n",
    "#from sklearn.metrics import mean_squared_error  # For MSE\n",
    "\n",
    "cv_MLR = LinearRegression()\n",
    "cv_results = cross_val_score(cv_MLR, X_trainCV, y_trainCV, cv = 10, scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "print(\"RMSE on folds: \", -1*cv_results)\n",
    "print(\"Cross validation RMSE estimate: \", (-1*cv_results).mean())\n",
    "#sorted(SCORERS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gui4CqQrbco8"
   },
   "source": [
    "From the output above, we can see each of the ten error estimates resulting from model fitting and validation on each of the ten folds. Notice that during each \"round\" of cross validation, the model was fit on 9 folds and then validated on the single fold which was left out. The error estimates came in as low as \\\\$5,638 and as high as \\\\$6,672. This shows how volatile the traditional *validation-set approach* can be. Through cross-validation we estimate a true prediction error near \\\\$6,174.\n",
    "\n",
    "We now run cross-validation on the model which includes curvature associated with the numeric `age` predictor only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG8Zs_7mbjhD"
   },
   "outputs": [],
   "source": [
    "X_trainQuadCV = pd.concat([X_train_quadAge, X_test_quadAge])\n",
    "\n",
    "cv_MLR_age2 = LinearRegression()\n",
    "cv_results = cross_val_score(cv_MLR_age2, X_trainQuadCV, y_trainCV, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "\n",
    "print(\"RMSE on folds: \", -1*cv_results)\n",
    "print(\"Cross validation RMSE estimate: \", (-1*cv_results).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkDn8MvfifQQ"
   },
   "source": [
    "We see the same printouts as in the cross-validation of the straight-line linear model, with the error estimates updated to reflect the new model which includes a term for `age`$^2$. The cross-validation error estimates an improved prediction error of \\$6,132.31. Cross-validation indicates that the increased flexibility from adding the `age`$^2$ term results in a true improvement of fit.\n",
    "\n",
    "Finally we will run cross-validation on our most flexible model -- the model that includes all higher-order terms between numerical predictors up to degree (and including) degree 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfXtjcYTcF7c"
   },
   "outputs": [],
   "source": [
    "X_train_polyCV = pd.concat([X_train_poly, X_test_poly])\n",
    "\n",
    "cv_MLR_poly = LinearRegression()  # Create a instance for Linear Regression model\n",
    "cv_results = cross_val_score(cv_MLR_poly, X_train_polyCV, y_trainCV, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "print(\"RMSE on folds: \", -1*cv_results)\n",
    "print(\"Cross validation RMSE estimate: \", (-1*cv_results).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoRtxluGdVPi"
   },
   "source": [
    "Again, exploring the cross-validation output shows us the ten individual error estimates as well as the final reported cross-validation error estimate. As expected, the cross-validation error indicates an overfit here, since the resulting cross-validation error is higher for this most flexible model than it was for the model which only included a curvature term associated with the `age` predictor.\n",
    "\n",
    "### Final Model Selection\n",
    "\n",
    "We ran cross-validation on each of our proposed models -- the straight line model, the model allowing curvature in the relationship between `age` and `charges`, and the model allowing for third degree terms (an extremely flexible model). Comparing the cross validation error rates leads us to believe that the model allowing curvature in `age` but requiring straight line relationships with all other predictors is expected to perform best on new data. We move forward with that model.\n",
    "\n",
    "### Validation using the `safe`\n",
    "\n",
    "Before we move forward and launch this model into the production phase, it is useful to perform one last validation. Let's compute the prediction error ($\\tt{RMSE}$) on the `safe` to be sure that the model continues to perform as expected. If our expectation from the cross-validation error and this last error estimate strongly disagree, we should do some more analysis before putting the model into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLQVuA0vifQV"
   },
   "outputs": [],
   "source": [
    "finalModel = LinearRegression()\n",
    "\n",
    "finalModel.fit(X_trainQuadCV, y_trainCV)\n",
    "\n",
    "safeRMSE = (((y_safe - finalModel.predict(X_safe_quadAge))**2).mean())**0.5\n",
    "\n",
    "print(\"The RMSE on the safe set is: $\", safeRMSE, sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_jXkFftifQY"
   },
   "source": [
    "The model performs much better than expected here! We must have had a really \"friendly\" safe set. To see more on strategically constructing, *training*, *test*, and *safe* sets that are of comparable \"difficulty\", look up `sklearn`'s `StratifiedShuffleSplit` method as an alternative to `train_test_split`.\n",
    "\n",
    "Since we are happy with the results of the final validation, let's re-fit the model on all of the available data and then look at putting the model into production!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sFtMzIhifQZ"
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_train_quadAge, X_test_quadAge, X_safe_quadAge])\n",
    "y = pd.concat([y_train, y_test, y_safe])\n",
    "\n",
    "productionChargesModel = LinearRegression()\n",
    "productionChargesModel.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkTA41Ysd9lY"
   },
   "source": [
    "## Using the Model for Prediction on a New Record\n",
    "\n",
    "Putting a model into production simply means using that model to make predictions on \"live\" data -- that is, observations for which the response is not yet known. Consider a 34 year old male from the Northeast, with two kids, a BMI of 28, and who is a non-smoker. How much do we expect this person to cost the insurance company in charges this year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n2uI21Xd32n"
   },
   "outputs": [],
   "source": [
    "#Let's remind ourselves of the columns in the dataset\n",
    "X_train_quadAge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eIJWhBoegpQ"
   },
   "outputs": [],
   "source": [
    "#Input the data for our new customer (we should be sure that variable names match)\n",
    "new = {\"age\" : [34], \"bmi\" : [28], \"hasChildren_moreThanThree\" : [0], \"hasChildren_oneToThree\" : [1], \"sex_male\" : [1], \"smoker_yes\" : [0], \"region_northwest\" : [0], \"region_southeast\" : [0], \"region_southwest\" : [0], \"age2\" : [34**2]}\n",
    "new = pd.DataFrame(new)\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9ulWv7kifQk"
   },
   "source": [
    "Now we use the model to make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnOocVQ-fC3r"
   },
   "outputs": [],
   "source": [
    "productionChargesModel.predict(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aNA5-XogVEg"
   },
   "source": [
    "We expect this individual to cost about \\$6,568.22 to insure next year. The insurance company can now use this information, in conjunction with what we know about the expected variation in these predictions in order to set a rate to charge this individual.\n",
    "\n",
    "## Summary\n",
    "\n",
    "I hope you found this workbook to be interesting and that it has opened your eyes to some advanced techniques. Part of what I hope you realize is that predictive modeling can be an art-form -- data visualization can really inform our model construction. Improving our prediction error by about \\\\$100 might not sound like much, but consider that our improvement applies to a large customer base -- maybe 100,000 or more individuls. This would be a \\\\$10,000,000 value to the insurance agency -- not too shabby!\n",
    "\n",
    "Improving our predictive models often involves increasing their flexibility. Do not be blinded, however -- increasing flexibility generally reduces the model's *training error* but does not necessarily reduce the true *prediction error*. Increases in flexibility also increase the likelihood of *overfitting* -- therefore, methods like *cross-validation* should be used to help aid in your model-selection process, since this reduces the dependence of the error rates on the particular training, test, and safe sets chosen. \n",
    "\n",
    "Next week we'll move to a discussion on tree-based models which have applications in both the regression and classification settings. See you then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9CAM6V9ifQp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "10_Regression_PartII.ipynb",
   "provenance": [
    {
     "file_id": "1b82w7y7JW2qq3zMQ0X9xs1xWf1GXstyR",
     "timestamp": 1578321565295
    },
    {
     "file_id": "1xedbseYti1b2aaeMUb7YsyOtny8I6oPF",
     "timestamp": 1573156909581
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
