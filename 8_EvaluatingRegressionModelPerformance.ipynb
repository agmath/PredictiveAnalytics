{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7_EvaluatingRegressionModelPerformance.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e490961GB2J0"},"source":["# Evaluating Predictive Performance for Regressors\n","\n","This notebook discusses different methods for evaluating predictive performance in the case of regression models. The discussion here accompanies pages 125 - 131 of our textbook. It is important to consider multiple metrics because each gives insight to different facets of our expected prediction errors.\n","\n","As usual, we'll start by loading the libraries we will use (it is good practice to load these all at once near the beginning of your analysis, so any reader knows what packages they'll need to run your notebook). As a reminder, we use `pandas` to manipulate data frames, `numpy` expands our ability for numerical computations, `matplotlib` and `seaborn` are used for plotting, we import `train_test_split` and `LinearRegression` methods from `sklearn` to split data into *training* and *test* sets, and to build linear regression models respectively, and finally we read in the `diamonds` data frame from a public GitHub repository."]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"kv9EGCHrBwgk","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","\n","#Install our textbook's library and some associated custom functions\n","!pip install dmba\n","from dmba import liftChart, gainsChart\n","\n","diamonds_df  = pd.read_csv(\"https://raw.githubusercontent.com/tidyverse/ggplot2/master/data-raw/diamonds.csv\")\n","diamonds_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzkMbR7RuqNv","colab_type":"text"},"source":["We see that `diamonds_df` has no missing data and contains three columns which are categorical (the `cut`, `color`, and `clarity` columns are all listed as *object*). Let's use *one-hot-encoding* to get dummy variables for these, and then we'll split our data into *training*, *test*, and *safe* sets. Notice how the width of the dataframe explodes when we add the dummy variables in."]},{"cell_type":"code","metadata":{"id":"1Ud6tFNUuqNx","colab_type":"code","colab":{}},"source":["print(diamonds_df.shape)\n","diamonds_df = pd.get_dummies(diamonds_df, [\"cut\", \"color\", \"clarity\"], drop_first = True)\n","print(diamonds_df.shape)\n","diamonds_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVtUKyIbuqOB","colab_type":"text"},"source":["Now that we have the data, we'll split it into *training*, *test*, and *safe* sets using `train_test_split()`. This is our first time using `train_test_split()`, since last time we built our own custom function. The `train_test_split()` function takes three arguments -- the first is a data frame of all of our features except the response, the second is a list or dataframe containing only the response variable, and the third is an argument for `test_size` (the default is 0.25 -- or, 25% of the available observations). In addition to these three arguments, we'll pass a value to the optional `random_state` parameter so that this notebook generates the same data for everyone. The output from `train_test_split()` is four distinct objects -- a data frame of training features, a data frame of the corresponding responses, a data frame of test features, and a data frame of the corresponding test responses. You'll also notice that we'll use `train_test_split()` twice since we want three subsets of data rather than just two."]},{"cell_type":"code","metadata":{"id":"TOhzM_5YuqOE","colab_type":"code","colab":{}},"source":["X = diamonds_df.drop([\"price\"], axis = 1)\n","y = diamonds_df[\"price\"]\n","\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.25, random_state = 370)\n","X_test, X_safe, y_test, y_safe = train_test_split(X_temp, y_temp, test_size = 0.6, random_state = 570)\n","print(X_train.head())\n","print(y_train.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02hdh9ScuqOQ","colab_type":"text"},"source":["I printed out the `X_train` and `y_train` dataframes so that you could see what happened. Notice the indices on the left of the printouts -- they are no longer in order, since the indices were shuffled while randomly being assigned to the *training* or *temp* sets. You can also see that `X_train` is a dataframe of the features and excludes the response column (`price`), while `y_train` is a single-column dataframe containing only the response variable.\n","\n","At this point in an analytics project, we would likely engage in data visualization, feature engineering, and dimension reduction. Since the goal of this notebook is to discuss error metrics and model validation, we will skip all that and just create a model for us to asses. We will keep things simple here and build a multiple regression model. Our model will include all of the available features (that's 24 terms -- one for each of the variables included in `X_train`, plus the intercept). The form of the model is as follows:\n","$$\\displaystyle{\\mathbb{E}\\left[y\\right] = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_{23}x_{23}}$$\n","The model is constructed via the code below -- see the comments for explanations."]},{"cell_type":"code","metadata":{"id":"2aA9xZe0uqOW","colab_type":"code","colab":{}},"source":["#Initialize a linear regressor\n","lin_reg = LinearRegression()\n","\n","#Fit the regressor on the training data\n","lin_reg.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q-h0jTKe9126"},"source":["## Assessing Regression Models\n","\n","Now that we have a regression model, it's time to start assessing it! Let's walk through some terms and definitions first (inlcuding the definitions of several error metrics), and then we'll experiement with assessing our regression model. \n","\n","+ **Benchmarking:** There are multiple ways we can benchmark a model. Perhaps the simplest reasonable benchmark is to just predict the average value for the response, regardless of the characteristics of the record. Any predictive model should beat the benchmark.\n","+ **Residual:** A residual is an unexplained error resulting from a model prediction. The residual for the $i^{th}$ record is labeled $e_i$ and is computed as the observed response minus the predicted response. That is,\n","$$e_i = y_i - \\hat{y_i}$$\n","\n","There are multiple metrics we can use to assess regression models. Each metric tells us something different about our expected model performance.\n","+ **Mean Error:** the average training error for our model should be near 0, but the sign on the average test error may tell us whether we should expect over- or under-predictions on average for new data.\n","$$ME = \\frac{1}{n}\\sum{\\left(y_i - \\hat{y_i}\\right)}$$\n","+ **Mean Absolute Error:** the average absolute training error provides us an estimate for the *magnitude* of the average error.\n","$$MAE = \\frac{1}{n}\\sum{\\left|y_i - \\hat{y_i}\\right|}$$\n","+ **Mean Percentage Error:** Gives the percentage the predictions <u>differ</u> (accounting for direction) from their actual values on average. Similar to mean error, we expect the MPE for training data to be near 0, but MPE for test data gives us more insight into true predictive performance.\n","$$MPE = 100\\times\\frac{1}{n}\\sum{\\left(y_i - \\hat{y_i}\\right)/y_i}$$\n","+ **Mean Absolute Percentage Error:** Gives the percentage the predictions <u>deviate</u> from their actual values on average.\n","$$MAPE = 100\\times\\frac{1}{n}\\sum{\\left|\\left(y_i - \\hat{y_i}\\right)/y_i\\right|}$$\n","+ **Root Mean Square Error:** This is the classical error metric since is is modeled after the standard deviation formula. This metric estimates the standard deviation for prediction errors.\n","$$RMSE = \\sqrt{\\frac{1}{n}\\sum{\\left(y_i - \\hat{y_i}\\right)^2}}$$\n","\n","Note that all of these metrics are aggregate summaries which can be strongly influenced by outliers. Since outliers can strongly skew these metrics we should also look at the distributions of these errors via *boxplots*, *histograms*, and *quantile-quantile* plots. We may also consider *median-based* analogs to these error metrics to determine the impact of outliers the error estimates.\n","\n","#### Assessing our model\n","\n","Since we built a regression model to predict the price of a diamond given its various characteristics, a natural next question is -- *how good did we do?* We start below with a benchmark. If we had no information about a diamond, what price should we predict? *The mean*. Let's build a benchmark model by computing the average price of a diamond in the *training set* and see how that model performs under our various metrics."]},{"cell_type":"code","metadata":{"id":"2ruYOa79uqOx","colab_type":"code","colab":{}},"source":["benchmark_model = y_train.mean()\n","print(\"The benchmark model predicts that the price of every diamond will be $\", benchmark_model, sep = \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5TojXxvuqPZ","colab_type":"text"},"source":["In the code block above, we built a benchmark model which uses no information about the diamond in question in order to make a prediction. The benchmark model simply predicts the average value (\\\\$3,927.56) for every single diamond. In the code block below, we compute the first five residuals (prediction errors) when the naive benchmark model is applied to the *training* and *test* data."]},{"cell_type":"code","metadata":{"id":"mM3T6tLKuqPb","colab_type":"code","colab":{}},"source":["print(\"The first five residuals when applied to the training data are:\\n\", (y_train - benchmark_model).head())\n","print(\"The first five residuals when applied to the test data are:\\n\", (y_test - benchmark_model).head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8AnFNo4LuqPl","colab_type":"text"},"source":["In the code block above, we printed the first five residuals illustrative purposes only. We typically do not care about individual residuals unless they are atypical (for example, a very large error).\n","\n","Now that we understand our benchmark model and the idea of *residuals* has been made more concrete, let's move to analyzing our error metrics. We'll start with the mean residual error ($\\tt{ME}$) below."]},{"cell_type":"code","metadata":{"id":"BK4oUGmFuqPm","colab_type":"code","colab":{}},"source":["print(\"The mean training error is $\", (y_train - benchmark_model).mean(), sep = \"\")\n","print(\"The mean test error is $\", (y_test - benchmark_model).mean(), sep = \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kd0PHBYwuqPw","colab_type":"text"},"source":["Recall from our development of the error metrics that we expected the average training error to be nearly 0, and that is the case here. The average test error is \\\\$75.10, which indicates that our model is expected to *underpredict* the price of new diamonds on average and in the long-run. Remember that a significant shortcoming of the average error metric is that *over-* and *underpredictions* tend to cancel one-another out. Since this is the case, the average error rate does not give us an idea of how large our \"typical\" error will be.\n","\n","In order to better understand the expected typical error, we turn to the *absolute* metrics. We first consider the *mean absolute error* ($\\tt{MAE}$)."]},{"cell_type":"code","metadata":{"id":"oqd-aAkruqQA","colab_type":"code","colab":{}},"source":["print(\"The mean absolute training error is $\", (abs(y_train - benchmark_model).mean()), sep = \"\")\n","print(\"The mean absolute test error is $\", (abs(y_test - benchmark_model).mean()), sep = \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xviyp02guqQI","colab_type":"text"},"source":["From the output above we can see that the average error made on a single diamond is \\\\$3,026.59 for a diamond in the training set and \\\\$3,061.75 for a diamond in the test set. The average error made for a diamond in the test set is slightly higher than the average error made for a diamond in the training set -- this is to be expected since the model was \"trained on\" the training data -- in some sense, the model knows the prices of the diamonds in the training set. We do note, however that this metric of the average error made being over \\\\$3,000 is quite bad since the average price of a diamond is slightly below \\\\$4,000. Again, this is expected since we are working with the *benchmark* model that uses no information about a diamond before making a prediction.\n","\n","We will skip the percentage errors here, but note that they are useful for helping in the cases where we suspect that errors made are correlated to the size of the target response. That is, if our model makes \"small\" errors for diamonds with a low price and makes \"large\" errors for diamonds with a high price. Instead, we skip directly to the most common metric for evaluation a regressor's performance -- the root mean square error ($\\tt{RMSE}$)."]},{"cell_type":"code","metadata":{"id":"za-QsD5KuqQJ","colab_type":"code","colab":{}},"source":["print(\"The root mean square training error is $\", ((((y_train - benchmark_model)**2).mean())**0.5), sep = \"\")\n","print(\"The root mean square test error is $\", ((((y_test - benchmark_model)**2).mean())**0.5), sep = \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VDZgcuThuqQR","colab_type":"text"},"source":["We see that the $\\tt{RMSE}$ metric measures a larger error than the mean absolute error ($\\tt{MAE}$) metric. This is because large residuals are weighted more heavily than small residuals in the $\\tt{RMSE}$ metric. We'll discuss more on the utility of $\\tt{RMSE}$ later. Essentially we can use it to build confidence and prediction intervals so that we are predicting a range of values rather than just a point estimate. The mathematics may look a bit intimidating, but I'll write a function for you that can be re-used simply by copying and pasting into your new Jupyter notebooks.\n","\n","### Comparing our model to the benchmark model\n","\n","As we mentioned, our predictive model should out-perform the benchmark model. Indeed, it actually uses some information about the diamond before predicting its price while the benchmark model just made a naive prediction. Let's see how our linear regression model compares to the benchmark. We go through the same series of error metrics as above, beginning with the mean error."]},{"cell_type":"code","metadata":{"id":"FER0XmY6uqQT","colab_type":"code","colab":{}},"source":["print(\"The mean training error is $\", (y_train - lin_reg.predict(X_train)).mean(), sep = \"\")\n","print(\"The mean test error is $\", (y_test - lin_reg.predict(X_test)).mean(), sep = \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwNFqCL3uqQZ","colab_type":"text"},"source":["Our linear regression model is expected to overpredict the prices of diamonds on average, in the long-run. The model does perform much better on the test data than the benchmark model did with respect to this metric -- we cut the magnitude of the benchmark error down to less than a third of what it was prior. We can also draw and compare distributions of the individual errors made by the *benchmark* and *linear regression* models. We see that the *mode* of the residuals in the case of our regression model is near 0, while the mode of the residuals in the case of the benchmark model is a negative value -- indicating that we are overpredicting price for the majority of our diamonds."]},{"cell_type":"code","metadata":{"id":"9JjoxGVyuqQb","colab_type":"code","colab":{}},"source":["benchmarkTestError = (y_test - benchmark_model).values\n","lin_regTestError = (y_test - lin_reg.predict(X_test))\n","\n","plt.figure(figsize = (12, 4))\n","\n","plt.subplot(1,2,1)\n","sns.distplot(benchmarkTestError)\n","plt.title(\"Benchmark Residuals\")\n","\n","plt.subplot(1,2,2)\n","sns.distplot(lin_regTestError)\n","plt.title(\"Linear Regression Residuals\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VnFwOkMuqQh","colab_type":"text"},"source":["We now move to $\\tt{MAE}$."]},{"cell_type":"code","metadata":{"id":"9p9agdo4uqQj","colab_type":"code","colab":{}},"source":["print(\"The mean absolute training error is $\", (abs(y_train - lin_reg.predict(X_train)).mean()), sep = \"\")\n","print(\"The mean absolute test error is $\", (abs(y_test - lin_reg.predict(X_test)).mean()), sep = \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U20MtPijuqQp","colab_type":"text"},"source":["Again, the linear regression model totally blows the benchmark model out of the water with respect to $\\tt{MAE}$. The average error made on a single prediction is just below \\\\$750 on both the training and test diamonds. Again, since the average price of a diamond is just under \\\\$4,000, this is not a spectacular result, but it is pretty good for a first-pass with no advanced techniques. We note that the test error is slightly higher than the training error, which is to be expected -- also, since these error metrics are close to one another, it is unlikely that we are *overfitting* our model, but we may be *underfitting* it. Again, we can build a plot to compare the distribution of the *absolute error* associated with individual observations for the benchmarking model as well as the linear regression model."]},{"cell_type":"code","metadata":{"id":"XMJgZqdSuqQq","colab_type":"code","colab":{}},"source":["benchmarkAbsoluteTestError = abs((y_test - benchmark_model)).values\n","lin_regAbsoluteTestError = abs(y_test - lin_reg.predict(X_test))\n","\n","plt.figure(figsize = (12, 4))\n","\n","plt.subplot(1,2,1)\n","sns.distplot(benchmarkAbsoluteTestError)\n","plt.title(\"Benchmark Absolute Residuals\")\n","\n","plt.subplot(1,2,2)\n","sns.distplot(lin_regAbsoluteTestError)\n","plt.title(\"Linear Regression Absolute Residuals\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GY3a-bBhuqQx","colab_type":"text"},"source":[" Not surprisingly, we see significantly worse performance on and individual basis by the benchmarking model. We now move to $\\tt{RMSE}$. We'll start by simply computing the metrics and producing the comparison plots."]},{"cell_type":"code","metadata":{"id":"IU6ZuZtwuqQy","colab_type":"code","colab":{}},"source":["print(\"The root mean square training error is $\", (((((y_train - lin_reg.predict(X_train))**2).mean())**0.5)), sep = \"\")\n","print(\"The root mean square test error is $\", (((((y_test - lin_reg.predict(X_test))**2).mean())**0.5)), sep = \"\")\n","\n","benchmarkSquaredErrorsTest = ((y_test - benchmark_model)**2)\n","lin_regSquaredErrorsTest = (y_test - lin_reg.predict(X_test))**2\n","\n","plt.figure(figsize = (12, 4))\n","\n","plt.subplot(1,2,1)\n","sns.distplot(benchmarkSquaredErrorsTest)\n","plt.title(\"Benchmark Squared Residuals\")\n","\n","plt.subplot(1,2,2)\n","sns.distplot(lin_regSquaredErrorsTest)\n","plt.title(\"Linear Regression Squared Residuals\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TrAe45HBuqQ0","colab_type":"text"},"source":["Again, we arrive at error metrics that are quite similar to one another. As expected, the $\\tt{RMSE}$ metric is larger than the $\\tt{MAE}$ metric, since large errors are more heavily weighted in its calculation. Still, an $\\tt{RMSE}$ around \\\\$1,100 is not so bad given that we haven't really tried to construct a good model. From the residual plots, we again see the superiority of the linear regression model over the naive benchmark.\n","\n","We notice that the $\\tt{RMSE}$ for the training data is higher than the $\\tt{RMSE}$ for the test data -- this is a good indication that our model is *underfitting* -- we should go back and try to improve our model -- perhaps using some curved relationships or allowing for interaction between predictors (is it possible that the expected increase in price due to carat weight is not the same for a diamond with a *fair* cut as it is for a diamond with an *ideal* cut?). We'll learn more about this when we cover regression analysis in detail. For now, let's think more about the RMSE metric and what it is used for."]},{"cell_type":"markdown","metadata":{"id":"FMSjvPXpuqQ1","colab_type":"text"},"source":["### More on the Root Mean Square Error (RMSE) and Computing Confidence and Prediction Intervals\n","\n","The root mean square error is a metric similar to the mean absolute error, but large errors are more severely penalized in $\\tt{RMSE}$ than they are in mean absolute error -- this is the case due to the squaring of errors that occurs as part of this metric. We see that the training $\\tt{RMSE}$ is \\\\$3,984.03 and the test $\\tt{RMSE}$ is \\\\$4,065.27 when considering the benchmark model, while the corresponding values for the linear regressor are \\\\$1,134.40 and \\\\$1,113.87 respectively. We can think of the $\\tt{RMSE}$ metric (in particular the test $\\tt{RMSE}$) as part of the margin of error of a confidence or prediction interval. If you remember from MAT240 or MAT241, a confidence interval can be computed as follows: $\\displaystyle{\\left(\\tt{point~estimate}\\right)\\pm \\underbrace{\\left(\\tt{critical~value}\\right)\\left(\\tt{standard~error}\\right)}_{\\text{Margin of Error}}}$. In the case of confidence and prediction intervals for regression models, we have slightly modified formulae.\n","\n","#### Confidence Intervals for the **Mean** Response\n","\n","If we wish to predict the average response for all observations satisfying certain criteria, then we can compute a confidence interval for that average response. That is, if we wish to produce the *average price among all 0.75 carat diamonds, with an ideal cut, having color J, etc.* then we can compute a confidence interval. As mentioned, the confidence interval formula is slightly modified from the one you were exposed to in MAT240/MAT241 -- it appears below.\n","$$\\displaystyle{\\left(\\tt{point~estimate}\\right)\\pm t^*_{n-k}\\left(\\tt{RMSE}\\right)\\left(\\sqrt{\\frac{1}{n} + \\sum_{j\\in \\tt{features}}{\\frac{\\left(x_j^* - \\bar{x_j}\\right)^2}{\\left(n-1\\right)s_{x_j}^2}}}\\right)}$$\n","\n","In the formula, the $\\tt{point~estimate}$ is the predicted response from the regression model, the critical value $t*_{n-k}$ is the critical value corresponding to the desired level of confidence in the $t$ distribution with $n-k$ degrees of freedom (where $k$ is the number of $\\beta$-coefficients estimated as part of the model -- recall that this is one more than the number of columns included in the training data), $n$ is the number of observations in the training set, $x_j$ is the $j^{th}$ feature of the observation, while $\\bar{x_j}$ is the average value of the $j^{th}$ feature over all observations in the training set, and $s_{x_{j}}$ is the standard deviation of the $j^{th}$ feature over all observations in the training set.\n","\n","While this formula may look very intimidating, it is worth noting that the only difference between it and the formula you learned in MAT240/MAT241 is $\\displaystyle{\\sqrt{\\frac{1}{n} + \\sum_{j\\in \\tt{features}}{\\frac{\\left(x^*_j - \\bar{x_j}\\right)^2}{\\left(n-1\\right)s_{x_j}^2}}}}$. The impact that this additional component has is that confidence intervals for observations which are \"close to average\" are more narrow, while confidence intervals for observations away from the mean become wider.\n","\n","#### Prediction Intervals for a **Single** Response\n","\n","Often times we are more interested in a single observed value than in the average expected response for all instances having similar characteristics. When we want to predict an *average* response, we benefit from the notion that unusually large and unusually small observations will \"cancel one another out\" -- so predicting an average is much easier than predicting a single response. This means that prediction intervals will be wider than confidence intervals since we don't benefit from the *Law of Averages*. The formula for a prediction interval is below.\n","$$\\displaystyle{\\left(\\tt{point~estimate}\\right)\\pm t^*_{n-k}\\left(\\tt{RMSE}\\right)\\left(\\sqrt{1 + \\frac{1}{n} + \\sum_{j\\in \\tt{features}}{\\frac{\\left(x^*_j - \\bar{x_j}\\right)^2}{\\left(n-1\\right)s_{x_j}^2}}}\\right)}$$\n","\n","The only difference between the confidence interval and prediction interval formulas is the additional 1 inside the square root which deals with the distance between the observation of interest and the *average observation*. The impact of this additional term is that it widens the interval to compensate for the fact that we are trying to predict the response associated with a single observation rather than the average over all similar observations.\n","\n","Since many of you likely don't want to be evaluating these formulas on your own, I'm writing a function in the code block below which you can copy and paste into your Jupyter notebooks so that you can evaluate confidence and prediction intervals more easily. You'll notice that the function requires part of the `scipy.stats` library, so don't forget to add `from scipy.stats import t` to the top of your notebook if you use this function. When you use this function you'll need to pass it the training features (`X_train`) and training responses (`y_train`) as well as the `model` being used to generate predictions, and the `new_data` you want to make predictions on -- there are additional arguments with pre-set defaults (`c_level` and `type`) which you can change as well. If you dig through the code, you'll also notice that I'm using a slightly different formula for $\\tt{RMSE}$ -- my formula provides an *unbiased* $\\tt{RMSE}$ estimate, penalizing models with additional terms -- this compensates for not only the uncertainty in predicting the response but also for uncertainty and errors in measuring the features as inputs. The second code block below uses the function to compute 90% confident prediction intervals for diamonds matching the description of the first 10 diamonds in the test set."]},{"cell_type":"code","metadata":{"id":"r-IBdNTvuqQ2","colab_type":"code","colab":{}},"source":["from scipy.stats import t\n","\n","def CalcInterval(X_train, y_train, model, new_data, c_level = 0.95, type = \"confidence\"):\n","    point_estimate = model.predict(new_data)\n","    RMSE = ((((model.predict(X_train) - y_train)**2).sum()/(X_train.shape[0] - (X_train.shape[1] + 1)))**0.5)\n","    crit_val = t.ppf(q = 1 - ((1-c_level)/2), df = (X_train.shape[0] - (X_train.shape[1] + 1)))\n","    if (type == \"prediction\"):\n","        from_Avg = (1 + (1/(X_train.shape[0])) + ((new_data - (X_train.mean()))**2/((X_train.shape[0] -1)*(np.std(X_train)**2)*(X_train.shape[0] - 1))).values.sum())\n","    else:\n","        from_Avg = ((1/(X_train.shape[0])) + ((new_data - (X_train.mean()))**2/((X_train.shape[0] -1)*(np.std(X_train)**2)*(X_train.shape[0] - 1))).values.sum())\n","    lower = (point_estimate - (crit_val*RMSE*from_Avg))\n","    upper = point_estimate + (crit_val*RMSE*from_Avg)\n","    df = pd.DataFrame()\n","    df[\"LowerBound\"] = lower\n","    df[\"UpperBound\"] = upper\n","\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6OBN791fuqQ7","colab_type":"code","colab":{}},"source":["CalcInterval(X_train, y_train, lin_reg, new_data = X_test.iloc[0:10, :], type = \"prediction\", c_level = 0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eaNbPcQjc9LE"},"source":["### Additional Metrics for Performance\n","\n","**Cumulative Gains and Lift Charts:** If the purpose of an application is to identify the most valuable of a set of records (say to flag products for individual rather than bulk sale), both cumulative gains and lift charts compare a model's predictive performance as an improvement over the baseline naive model. Let's say that we have an opportunity to sell off a bunch of our diamonds at a bulk price -- are we better off just including all of our diamonds in the bulk sale, or should we keep the most valuable diamonds out to sell at a fair market rate? Let's see!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9skqJ1wzfN-h","colab":{}},"source":["#Build lift chart here\n","pred_price = pd.Series(lin_reg.predict(X_test))\n","pred_price = pred_price.sort_values(ascending=False)\n","\n","fig, axes = plt.subplots(nrows=1, ncols=2)\n","#Build a gains chart (note that gainsChart() is part of our textbook -- dbma -- library)\n","ax = gainsChart(pred_price, ax=axes[0])\n","ax.set_ylabel('Cumulative Price')\n","ax.set_title('Cumulative Gains Chart')\n","\n","ax = liftChart(pred_price, ax=axes[1], labelBars=False)\n","ax.set_ylabel('Lift')\n","plt.axhline(y = 1, xmin = 0, xmax = 1, color = \"purple\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# group the sorted predictions into 10 roughly equal groups and calculate the mean\n","groups = [int(10 * i / len(pred_price)) for i in range(len(pred_price))]\n","\n","meanPercentile = pred_price.groupby(groups).mean()\n","meanResponse = meanPercentile / pred_price.mean()\n","print('First decile lift based on meanResponse', meanResponse[0])\n","random10 = pred_price.cumsum().iloc[-1] / 10  # expected cumulative price for random 10% of diamonds\n","cumPred10 = pred_price.cumsum().iloc[int(0.1*len(pred_price)) - 1]  # cumulative price based on model for top 10%\n","print('Expected cumulative price for 10% random sales', random10)\n","print('Cumulative price for top 10% sales', cumPred10)\n","print('Lift calculated based on gains chart', cumPred10 / random10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tMrZiuPVsvU4"},"source":["The plot on the top left shows the *cumulative gains* made from selling the diamonds and receiving the true price listed in the test data. The diamonds are ordered in the cumulative gains chart from highest predicted value (leftmost) to lowest predicted value (rightmost). In this way, we see the gains made over the expected gains from selling random diamonds (represented by the dotted line), where we would expect average gains (ie. we sell 100 diamonds, we expect revenue at `100*avg_value`). For those with a calculus background, the Mean Value Theorem helps us identify the optimum number of prioritized diamonds to sell individually rather than at a bulk rate.\n","\n","The plot on the right shows the *decile lifts*. From this plot we can identify how effectively our model flags high value diamonds. This plot gives us diamonds grouped in deciles (groups containing 10 percent of all the diamonds). We can see that if we sold the diamonds in our first grouping individually we would earn about 315% of the expected revenue from selling a random 10 percent of our diamonds. In fact, selling the best 40% of our diamonds (as flagged by our model) lead to higher than average gains.\n","\n","## Summary\n","\n","Okay, that's plenty for now. Here's a recap of what we learned. \n","+ Regression models predict a numerical response.\n","+ There are multiple ways to measure how good (or bad) our predictions are.\n","    + Every prediction results in a *residual*, or prediction error associated with the corresponding observation.\n","    + The *mean error* (or mean residual) should be near 0 and tells us whether we should expect overprediction or underprediction from our model in the long-run.\n","    + The *mean absolute error* measures the average of the absolute deviations from the error -- that is, we take the absolute value of the residuals before computing their average -- this gives us an idea of how large a \"typical\" error might be.\n","    + The *root mean square error* measures an average deviation from the mean but does so in a way that penalizes large errors more heavily than small errors. The root mean square error is the most common error metric for evaluating regression models because it can be used to construct **confidence intervals** (for the average response from a class of observations satisfying a set of characteristics) and **prediction intervals** (for a single response associated with an observation having specific characteristics).\n","    + In addition to the error metrics listed above, we also have *mean percentage error* and *mean absolute percentage error* which are useful metrics in the case where the magnitude of our residuals is correlated to the magnitude of the target response -- (ie. our model may make large errors when the target response is large and small errors when the target response is small).\n","+ Sometimes it is useful to use our model to rank high- or low-value observations in some sort of prioritization scheme. We may want to hold any suspected high value items out of a bulk sale opportunity like we discussed here with the diamonds. In such cases, we look at *cumulative gains* and *lift charts*.\n","\n","We'll be back next week to discuss evaluating performance of classification models. With the rest of your time this week, catch up on any previous work you've fallen behind on or haven't yet gotten to, and complete our next assignment."]},{"cell_type":"code","metadata":{"id":"m1U88Q35zG9W","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}